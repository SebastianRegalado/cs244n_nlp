## Q1 (a) 
(3 points) Show that the naive-softmax loss given in Equation (2) is the same as the cross-entropy loss between $\boldsymbol{y}$ and $\hat{\boldsymbol{y}}$; i.e., show that
$$
-\sum_{w \in \text { Vocab }} y_w \log \left(\hat{y}_w\right)=-\log \left(\hat{y}_o\right) .
$$
Your answer should be one line.

**Solution**
It is easy to see that this holds as $y_w = 1$ if and only if $w = o$.

## Q1 (b) 
(5 points) Compute the partial derivative of $\boldsymbol{J}_{\text {naive-softmax }}\left(\boldsymbol{v}_c, o, \boldsymbol{U}\right)$ with respect to $\boldsymbol{v}_c$. Please write your answer in terms of $\boldsymbol{y}, \hat{\boldsymbol{y}}$, and $\boldsymbol{U}$. Note that in this course, we expect your final answers to follow the shape convention. This means that the partial derivative of any function $f(x)$ with respect to $x$ should have the same shape as $x$. For this subpart, please present your answer in vectorized form. In particular, you may not refer to specific elements of $\boldsymbol{y}, \hat{\boldsymbol{y}}$, and $\boldsymbol{U}$ in your final answer (such as $\boldsymbol{y}_1, \boldsymbol{y}_2$, $\ldots$ ).

**Solution**
$$\frac{\partial}{\partial \boldsymbol{v}_c} \boldsymbol{J}_{\text {naive-softmax }}\left(\boldsymbol{v}_c, o, \boldsymbol{U}\right) = -Uy + U\hat{y}$$
## Q1 (c)

(5 points) Compute the partial derivatives of $\boldsymbol{J}_{\text {naive-softmax }}\left(\boldsymbol{v}_c, o, \boldsymbol{U}\right)$ with respect to each of the 'outside' word vectors, $\boldsymbol{u}_w$ 's. There will be two cases: when $w=o$, the true 'outside' word vector, and $w \neq o$, for all other words. Please write your answer in terms of $\boldsymbol{y}, \hat{\boldsymbol{y}}$, and $\boldsymbol{v}_c$. In this subpart, you may use specific elements within these terms as well, such as $\left(\boldsymbol{y}_1, \boldsymbol{y}_2, \ldots\right)$

**Solution**
$$\frac{\partial}{\partial \boldsymbol{u}_w} \boldsymbol{J}_{\text {naive-softmax }}\left(\boldsymbol{v}_c, o, \boldsymbol{U}\right) = -v_c[\boldsymbol{u}_w=\boldsymbol{u}_o] + \boldsymbol{v}_c\hat{\boldsymbol{y}}_w = -\boldsymbol{v}_c ([\boldsymbol{u}_w=\boldsymbol{u}_o] - \hat{\boldsymbol{y}}_w)$$
## Q1 (d)
(1 point) Compute the partial derivative of $\boldsymbol{J}_{\text {naive-softmax }}\left(\boldsymbol{v}_c, o, \boldsymbol{U}\right)$ with respect to $U$. Please write your answer in terms of $\frac{\partial \boldsymbol{J}\left(\boldsymbol{v}_c, o, \boldsymbol{U}\right)}{\partial \boldsymbol{u}_1}, \frac{\partial \boldsymbol{J}\left(\boldsymbol{v}_c, o, \boldsymbol{U}\right)}{\partial \boldsymbol{u}_2}, \ldots, \frac{\partial \boldsymbol{J}\left(\boldsymbol{v}_c, o, \boldsymbol{U}\right)}{\partial \boldsymbol{u}_{|V o c a b|}}$. The solution should be one or two lines long.

**Solution**
$$\frac{\partial}{\partial \boldsymbol{U}} \boldsymbol{J}_{\text {naive-softmax }}\left(\boldsymbol{v}_c, o, \boldsymbol{U}\right) = \hat{\boldsymbol{y}}^T \boldsymbol{v}_c - y_o\boldsymbol{v}_c (\boldsymbol{1}\circ y)^T$$

## Q1 (e)
(3 Points) The sigmoid function is given by Equation 4 :
$$
\sigma(x)=\frac{1}{1+e^{-x}}=\frac{e^x}{e^x+1}
$$
Please compute the derivative of $\sigma(x)$ with respect to $x$, where $x$ is a scalar. Hint: you may want to write your answer in terms of $\sigma(x)$.

**Solution**
$$\frac{\partial}{\partial x}\sigma(x) = \sigma(x)(1-\sigma(x))$$
## Q1 (f)
(4 points) Now we shall consider the Negative Sampling loss, which is an alternative to the Naive Softmax loss. Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall refer to them as $w_1, w_2, \ldots, w_K$ and their outside vectors as $\boldsymbol{u}_1, \ldots, \boldsymbol{u}_K$. For this question, assume that the $K$ negative samples are distinct. In other words, $i \neq j$ implies $w_i \neq w_j$ for $i, j \in\{1, \ldots, K\}$. Note that $o \notin\left\{w_1, \ldots, w_K\right\}$. For a center word $c$ and an outside word $o$, the negative sampling loss function is given by:
$$
\boldsymbol{J}_{\mathrm{neg} \text {-sample }}\left(\boldsymbol{v}_c, o, \boldsymbol{U}\right)=-\log \left(\sigma\left(\boldsymbol{u}_o^{\top} \boldsymbol{v}_c\right)\right)-\sum_{k=1}^K \log \left(\sigma\left(-\boldsymbol{u}_k^{\top} \boldsymbol{v}_c\right)\right)
$$
for a sample $w_1, \ldots w_K$, where $\sigma(\cdot)$ is the sigmoid function. ${ }^4$
Please repeat parts (b) and (c), computing the partial derivatives of $\boldsymbol{J}_{\text {neg-sample }}$ with respect to $\boldsymbol{v}_c$, with respect to $\boldsymbol{u}_o$, and with respect to a negative sample $\boldsymbol{u}_k$. Please write your answers in terms of the vectors $\boldsymbol{u}_o, \boldsymbol{v}_c$, and $\boldsymbol{u}_k$, where $k \in[1, K]$. After you've done this, describe with one sentence why this loss function is much more efficient to compute than the naive-softmax loss. Note, you should be able to use your solution to part (e) to help compute the necessary gradients here.

**Solution:**
Equivalent to (b):
$$\frac{\partial}{\partial \boldsymbol{v}_c} \boldsymbol{J}_{\mathrm{neg} \text {-sample }} \left(\boldsymbol{v}_c, o, \boldsymbol{U}\right) = -(1-\sigma\left(\boldsymbol{u}_o^{\top} \boldsymbol{v}_c\right))\boldsymbol{u}_o + \sum_{k=1}^K (1-\sigma\left(-\boldsymbol{u}_k^{\top} \boldsymbol{v}_c\right))\boldsymbol{u}_k$$
Equivalent to (c):
$$\frac{\partial}{\partial \boldsymbol{u}_w} \boldsymbol{J}_{\mathrm{neg} \text {-sample }} \left(\boldsymbol{v}_c, o, \boldsymbol{U}\right) = -(1-\sigma\left(\boldsymbol{u}_o^{\top} \boldsymbol{v}_c\right))\boldsymbol{v}_c[\boldsymbol{u_0 = u_w}] + \sum_{k=1}^K (1-\sigma\left(-\boldsymbol{u}_k^{\top} \boldsymbol{v}_c\right))\boldsymbol{v}_c[\boldsymbol{u_k = u_w}]$$

## Q1 (g)
(2 point) Now we will repeat the previous exercise, but without the assumption that the $K$ sampled words are distinct. Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall refer to them as $w_1, w_2, \ldots, w_K$ and their outside vectors as $\boldsymbol{u}_1, \ldots, \boldsymbol{u}_K$. In this question, you may not assume that the words are distinct. In other words, $w_i=w_j$ may be true when $i \neq j$ is true. Note that $o \notin\left\{w_1, \ldots, w_K\right\}$. For a center word $c$ and an outside word $o$, the negative sampling loss function is given by:
$$
\boldsymbol{J}_{\text {neg-sample }}\left(\boldsymbol{v}_c, o, \boldsymbol{U}\right)=-\log \left(\sigma\left(\boldsymbol{u}_o^{\top} \boldsymbol{v}_c\right)\right)-\sum_{k=1}^K \log \left(\sigma\left(-\boldsymbol{u}_k^{\top} \boldsymbol{v}_c\right)\right)
$$
for a sample $w_1, \ldots w_K$, where $\sigma(\cdot)$ is the sigmoid function.
Compute the partial derivative of $\boldsymbol{J}_{\text {neg-sample }}$ with respect to a negative sample $\boldsymbol{u}_k$. Please write your answers in terms of the vectors $\boldsymbol{v}_c$ and $\boldsymbol{u}_k$, where $k \in[1, K]$. Hint: break up the sum in the loss function into two sums: a sum over all sampled words equal to $u_k$ and a sum over all sampled words not equal to $u_k$.

**Solution:** Same as previous part

## Q1 (h)
(3 points) Suppose the center word is $c=w_t$ and the context window is $\left[w_{t-m}, \ldots, w_{t-1}, w_t, w_{t+1}\right.$, $\left.\ldots, w_{t+m}\right]$, where $m$ is the context window size. Recall that for the skip-gram version of word2vec, the total loss for the context window is:
$$
\boldsymbol{J}_{\text {skip-gram }}\left(\boldsymbol{v}_c, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right)=\sum_{\substack{-m \leq j \leq m \\ j \neq 0}} \boldsymbol{J}\left(\boldsymbol{v}_c, w_{t+j}, \boldsymbol{U}\right)
$$
Here, $\boldsymbol{J}\left(\boldsymbol{v}_c, w_{t+j}, \boldsymbol{U}\right)$ represents an arbitrary loss term for the center word $c=w_t$ and outside word $w_{t+j} . \boldsymbol{J}\left(\boldsymbol{v}_c, w_{t+j}, \boldsymbol{U}\right)$ could be $\boldsymbol{J}_{\text {naive-softmax }}\left(\boldsymbol{v}_c, w_{t+j}, \boldsymbol{U}\right)$ or $\boldsymbol{J}_{\text {neg-sample }}\left(\boldsymbol{v}_c, w_{t+j}, \boldsymbol{U}\right)$, depending on your implementation.
Write down three partial derivatives:
(i) $\partial \boldsymbol{J}_{\text {skip-gram }}\left(\boldsymbol{v}_c, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{U}$
(ii) $\partial \boldsymbol{J}_{\text {skip-gram }}\left(\boldsymbol{v}_c, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{v}_c$
(iii) $\partial \boldsymbol{J}_{\text {skip-gram }}\left(\boldsymbol{v}_c, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{v}_w$ when $w \neq c$
Write your answers in terms of $\partial \boldsymbol{J}\left(\boldsymbol{v}_c, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{U}$ and $\partial \boldsymbol{J}\left(\boldsymbol{v}_c, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{v}_c$. This is very simple each solution should be one line.

Once you're done: Given that you computed the derivatives of $\boldsymbol{J}\left(\boldsymbol{v}_c, w_{t+j}, \boldsymbol{U}\right)$ with respect to all the model parameters $\boldsymbol{U}$ and $\boldsymbol{V}$ in parts (a) to (c), you have now computed the derivatives of the full loss function $\boldsymbol{J}_{\text {skip-gram }}$ with respect to all parameters. You're ready to implement word2vec!

**Solution:**
(i) $$\partial \boldsymbol{J}_{\text {skip-gram }}\left(\boldsymbol{v}_c, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{U} = \sum_{\substack{-m \leq j \leq m \\ j \neq 0}} \partial \boldsymbol{J}\left(\boldsymbol{v}_c, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{U}$$
(ii) $$\partial \boldsymbol{J}_{\text {skip-gram }}\left(\boldsymbol{v}_c, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{v}_c = \sum_{\substack{-m \leq j \leq m \\ j \neq 0}} \partial \boldsymbol{J}\left(\boldsymbol{v}_c, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{v}_c$$
(iii) $$\partial \boldsymbol{J}_{\text {skip-gram }}\left(\boldsymbol{v}_c, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{v}_w = \boldsymbol{0}$$ when $w \neq c$
